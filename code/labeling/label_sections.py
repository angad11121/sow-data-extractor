"""
label_sections.py

Labels sections of SOW documents with canonical T (subtopic) and M (main topic) labels.
Uses the hierarchical taxonomy generated by consolidate_llm_topics.py.

This script:
1. Splits each SOW into semantic sections (paragraphs)
2. Uses embedding similarity to match sections to topics
3. Assigns both T and M labels to each section
4. Outputs structured JSON with labeled sections per document

Usage:
    python label_sections.py
"""

import os
import sys
import json
import glob
import asyncio
import numpy as np
from typing import Dict, List, Tuple, Optional
from sentence_transformers import SentenceTransformer
from tqdm.asyncio import tqdm



# ============================================================================
# CONFIGURATION
# ============================================================================
EMBEDDING_MODEL = "all-mpnet-base-v2"  # Better model, more accurate but slower
LLM_MODEL = "gpt-4.1-mini"
SIMILARITY_THRESHOLD = 0.38  # Minimum similarity to consider a topic match
TOP_K_TOPICS = 3  # Consider top K matching topics per section
MIN_SECTION_LENGTH = 30  # Minimum characters for a section to be considered

# ============================================================================
# GLOBAL VARIABLES (Runtime Configuration)
# ============================================================================
# Get the project root directory (2 levels up from this file)
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
TAXONOMY_FILE = os.path.join(PROJECT_ROOT, "extracted_topics/hierarchical_topics_full.json")
INPUT_DIR = os.path.join(PROJECT_ROOT, "generated_data")
OUTPUT_FILE = os.path.join(PROJECT_ROOT, "extracted_topics/labeled_sections.json")
LIMIT = 10  # Set to None to process all files, or a number to limit



class SectionLabeler:
    """Labels SOW document sections with canonical topics."""
    
    def __init__(self, taxonomy_file: str):
        """
        Initialize the labeler with taxonomy.
        
        Args:
            taxonomy_file: Path to hierarchical_topics_full.json
        """
        print("Loading taxonomy and embedding model...")
        
        # Load taxonomy
        with open(taxonomy_file, 'r') as f:
            data = json.load(f)
            self.taxonomy = data['taxonomy']
            self.main_topics = data['main_topics']
        
        # Initialize embedding model
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL)
        
        # Build topic lookup structures
        self._build_topic_index()
        
        print(f"âœ“ Loaded {len(self.main_topics)} main topics")
        print(f"âœ“ Loaded {len(self.subtopic_embeddings)} subtopics with embeddings")
    
    def _build_topic_index(self):
        """Build index of subtopic embeddings and metadata."""
        self.subtopic_ids = []
        self.subtopic_embeddings = []
        self.subtopic_info = {}
        
        for topic_id, topic_data in self.taxonomy.items():
            if topic_data['type'] == 'subtopic' and 'embedding' in topic_data:
                self.subtopic_ids.append(topic_id)
                self.subtopic_embeddings.append(topic_data['embedding'])
                
                # Store metadata for quick lookup
                self.subtopic_info[topic_id] = {
                    'name': topic_data['canonical_name'],
                    'description': topic_data['description'],
                    'parent': topic_data['parent'],
                    'parent_name': self.taxonomy[topic_data['parent']]['canonical_name']
                }
        
        # Convert to numpy array for efficient similarity computation
        self.subtopic_embeddings = np.array(self.subtopic_embeddings)
    
    def split_into_sections(self, text: str) -> List[Dict]:
        """
        Split document text into meaningful sections.
        
        Args:
            text: Full document text
            
        Returns:
            List of section dicts with text and metadata
        """
        sections = []
        
        # Split by double newlines (paragraphs)
        raw_paragraphs = text.split('\n\n')
        
        for idx, para in enumerate(raw_paragraphs):
            para = para.strip()
            
            # Filter out very short sections
            if len(para) < MIN_SECTION_LENGTH:
                continue
            
            # Check if it's a header-like section (short, ends with colon, or all caps)
            is_header = (
                len(para) < 100 and 
                (para.endswith(':') or para.isupper() or '\n' not in para)
            )
            
            sections.append({
                'section_idx': idx,
                'text': para,
                'is_header': is_header,
                'char_length': len(para)
            })
        
        return sections
    
    def find_matching_topics(self, section_text: str, top_k: int = TOP_K_TOPICS) -> List[Tuple[str, float]]:
        """
        Find topics matching a section using embedding similarity.
        
        Args:
            section_text: Text of the section
            top_k: Number of top matches to return
            
        Returns:
            List of (topic_id, similarity_score) tuples
        """
        # Embed the section
        section_embedding = self.embedding_model.encode(section_text)
        
        # Compute cosine similarities with all subtopics
        similarities = np.dot(self.subtopic_embeddings, section_embedding) / (
            np.linalg.norm(self.subtopic_embeddings, axis=1) * np.linalg.norm(section_embedding)
        )
        
        # Get top K matches above threshold
        top_indices = np.argsort(similarities)[::-1][:top_k]
        matches = [
            (self.subtopic_ids[idx], float(similarities[idx]))
            for idx in top_indices
            if similarities[idx] >= SIMILARITY_THRESHOLD
        ]
        
        return matches
    
    def label_section(self, section: Dict) -> Dict:
        """
        Label a single section with topic information.
        
        Args:
            section: Section dict with text
            
        Returns:
            Updated section dict with labels
        """
        # Find matching topics
        matches = self.find_matching_topics(section['text'])
        
        # If no matches, mark as unlabeled
        if not matches:
            section['labels'] = {
                'primary_subtopic': None,
                'primary_main_topic': None,
                'all_matches': [],
                'confidence': 0.0
            }
            return section
        
        # Primary match (highest similarity)
        primary_subtopic_id, primary_score = matches[0]
        primary_subtopic = self.subtopic_info[primary_subtopic_id]
        
        # Build label information
        section['labels'] = {
            'primary_subtopic': {
                'id': primary_subtopic_id,
                'name': primary_subtopic['name'],
                'confidence': primary_score
            },
            'primary_main_topic': {
                'id': primary_subtopic['parent'],
                'name': primary_subtopic['parent_name']
            },
            'all_matches': [
                {
                    'subtopic_id': tid,
                    'subtopic_name': self.subtopic_info[tid]['name'],
                    'main_topic_id': self.subtopic_info[tid]['parent'],
                    'main_topic_name': self.subtopic_info[tid]['parent_name'],
                    'confidence': score
                }
                for tid, score in matches
            ]
        }
        
        return section
    
    def label_document(self, doc_text: str, doc_id: str) -> Dict:
        """
        Label all sections in a document.
        
        Args:
            doc_text: Full document text
            doc_id: Document identifier
            
        Returns:
            Document dict with labeled sections
        """
        # Split into sections
        sections = self.split_into_sections(doc_text)
        
        # Label each section
        labeled_sections = [self.label_section(section) for section in sections]
        
        # Compute document-level statistics
        labeled_count = sum(1 for s in labeled_sections if s['labels']['primary_subtopic'] is not None)
        
        # Get topic coverage (which topics appear in this document)
        subtopics_present = set()
        main_topics_present = set()
        
        for section in labeled_sections:
            if section['labels']['primary_subtopic']:
                subtopics_present.add(section['labels']['primary_subtopic']['id'])
                main_topics_present.add(section['labels']['primary_main_topic']['id'])
        
        return {
            'doc_id': doc_id,
            'num_sections': len(labeled_sections),
            'num_labeled_sections': labeled_count,
            'coverage_pct': (labeled_count / len(labeled_sections) * 100) if labeled_sections else 0,
            'subtopics_present': sorted(list(subtopics_present)),
            'main_topics_present': sorted(list(main_topics_present)),
            'sections': labeled_sections
        }


def convert_to_minimal_format(labeled_docs: Dict, labeler: SectionLabeler) -> Dict:
    """
    Convert detailed labeled documents to minimal hierarchical format.
    
    Format:
    {
      "filename": {
        "M01": {
          "name": "Contract Terms",
          "subtopics": {
            "T001": {
              "name": "Effective Dates",
              "sections": [
                {"lines": "3-8", "text": "Scope of Work\nThe vendor will..."},
                {"lines": "45-52", "text": "Additional scope items..."}
              ]
            },
            "T002": {...}
          }
        },
        "M02": {...},
        "Others": {
          "unlabeled": [
            {"lines": "100-105", "text": "Some unclassified content..."}
          ]
        }
      }
    }
    
    Args:
        labeled_docs: Full labeled documents from label_document()
        labeler: SectionLabeler instance (to get topic names)
        
    Returns:
        Minimal hierarchical format
    """
    minimal = {}
    
    for doc_id, doc_data in labeled_docs.items():
        # Initialize document structure
        minimal[doc_id] = {}
        
        # Track line numbers (cumulative across sections)
        current_line = 1
        
        for section in doc_data['sections']:
            # Calculate line range for this section
            num_lines = section['text'].count('\n') + 1
            line_range = f"{current_line}-{current_line + num_lines - 1}"
            
            # Truncate long text for preview
            text_preview = section['text'][:200] + "..." if len(section['text']) > 200 else section['text']
            
            section_entry = {
                "lines": line_range,
                "text": text_preview
            }
            
            # Get labels
            labels = section['labels']
            
            # Check if we have valid labels with all required fields
            try:
                if (labels['primary_subtopic'] is None or 
                    labels['primary_main_topic'] is None):
                    raise ValueError("Missing topic labels")
                
                # Extract IDs and names (will raise KeyError if missing)
                main_id = labels['primary_main_topic']['id']
                main_name = labels['primary_main_topic']['name']
                subtopic_id = labels['primary_subtopic']['id']
                subtopic_name = labels['primary_subtopic']['name']
                
                # Validate that IDs and names are not None or empty
                if not main_id or not main_name or not subtopic_id or not subtopic_name:
                    raise ValueError("Empty topic ID or name")
                
                # Create main topic entry if doesn't exist
                if main_id not in minimal[doc_id]:
                    minimal[doc_id][main_id] = {
                        "name": main_name,
                        "subtopics": {}
                    }
                
                # Create subtopic entry if doesn't exist
                if subtopic_id not in minimal[doc_id][main_id]["subtopics"]:
                    minimal[doc_id][main_id]["subtopics"][subtopic_id] = {
                        "name": subtopic_name,
                        "sections": []
                    }
                
                # Add section
                minimal[doc_id][main_id]["subtopics"][subtopic_id]["sections"].append(section_entry)
                
            except (KeyError, ValueError, TypeError):
                # If any required field is missing or invalid, treat as unlabeled
                if "Others" not in minimal[doc_id]:
                    minimal[doc_id]["Others"] = {"unlabeled": []}
                minimal[doc_id]["Others"]["unlabeled"].append(section_entry)
            
            # Update line counter
            current_line += num_lines
    
    return minimal


def save_minimal_format(labeled_docs: Dict, labeler: SectionLabeler, output_file: str):
    """
    Save labeled documents in minimal hierarchical format.
    
    Args:
        labeled_docs: Full labeled documents
        labeler: SectionLabeler instance
        output_file: Output JSON file
    """
    print("\n" + "=" * 80)
    print("CONVERTING TO MINIMAL FORMAT")
    print("=" * 80)
    
    minimal = convert_to_minimal_format(labeled_docs, labeler)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(minimal, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… Minimal format saved to: {output_file}")
    
    # Print statistics
    total_sections = 0
    total_unlabeled = 0
    
    for doc in minimal.values():
        for main_topic_id, main_topic_data in doc.items():
            if main_topic_id == "Others":
                total_unlabeled += len(main_topic_data.get("unlabeled", []))
            else:
                for subtopic_data in main_topic_data.get("subtopics", {}).values():
                    total_sections += len(subtopic_data.get("sections", []))
    
    print(f"   Total labeled sections: {total_sections}")
    print(f"   Total unlabeled (Others): {total_unlabeled}")
    
    # Show sample structure
    print("\nðŸ“‹ SAMPLE MINIMAL FORMAT (first document):")
    first_doc_id = list(minimal.keys())[0]
    first_doc = minimal[first_doc_id]
    
    print(f"\nDocument: {first_doc_id}")
    for main_id in sorted(first_doc.keys()):
        if main_id == "Others":
            unlabeled_count = len(first_doc[main_id].get("unlabeled", []))
            print(f"  {main_id}:")
            print(f"    unlabeled: {unlabeled_count} sections")
        else:
            main_name = first_doc[main_id].get("name", "Unknown")
            print(f"  {main_id} ({main_name}):")
            for subtopic_id, subtopic_data in sorted(first_doc[main_id].get("subtopics", {}).items()):
                subtopic_name = subtopic_data.get("name", "Unknown")
                sections = subtopic_data.get("sections", [])
                print(f"    {subtopic_id} ({subtopic_name}): {len(sections)} sections")
                if sections:
                    print(f"      Example: {sections[0]['lines']} - {sections[0]['text'][:80]}...")


async def process_all_documents(
    input_dir: str,
    taxonomy_file: str,
    output_file: str,
    limit: Optional[int] = None
):
    """
    Process all SOW documents and label their sections.
    
    Args:
        input_dir: Directory containing SOW files
        taxonomy_file: Path to hierarchical taxonomy JSON
        output_file: Output file for labeled documents
        limit: Optional limit on number of files to process
    """
    print("=" * 80)
    print("SECTION LABELING PIPELINE")
    print("=" * 80)
    
    # Initialize labeler
    labeler = SectionLabeler(taxonomy_file)
    
    # Find all SOW files
    pattern = os.path.join(input_dir, "sow_*.txt")
    all_files = sorted(glob.glob(pattern))
    
    if limit:
        all_files = all_files[:limit]
    
    print(f"\nFound {len(all_files)} SOW files to process")
    print(f"Similarity threshold: {SIMILARITY_THRESHOLD}")
    print(f"Top-K matches per section: {TOP_K_TOPICS}")
    print("=" * 80)
    
    # Process files
    results = {}
    
    print("\nProcessing documents...")
    for filepath in tqdm(all_files, desc="Labeling sections"):
        doc_id = os.path.basename(filepath).replace('.txt', '')
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                doc_text = f.read()
            
            doc_result = labeler.label_document(doc_text, doc_id)
            results[doc_id] = doc_result
            
        except Exception as e:
            print(f"Error processing {filepath}: {e}")
            continue
    
    # Save minimal hierarchical format
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    minimal_output = output_file.replace('.json', '_minimal.json')
    save_minimal_format(results, labeler, minimal_output)
    
    # Print statistics
    print("\n" + "=" * 80)
    print("ðŸ“Š STATISTICS")
    print("=" * 80)
    
    total_sections = sum(doc['num_sections'] for doc in results.values())
    total_labeled = sum(doc['num_labeled_sections'] for doc in results.values())
    avg_coverage = np.mean([doc['coverage_pct'] for doc in results.values()])
    
    print(f"Documents processed: {len(results)}")
    print(f"Total sections: {total_sections}")
    print(f"Sections labeled: {total_labeled} ({total_labeled/total_sections*100:.1f}%)")
    print(f"Average coverage per doc: {avg_coverage:.1f}%")
    
    # Topic coverage across all documents
    all_subtopics = set()
    all_main_topics = set()
    for doc in results.values():
        all_subtopics.update(doc['subtopics_present'])
        all_main_topics.update(doc['main_topics_present'])
    
    print(f"\nUnique subtopics found: {len(all_subtopics)} / {len(labeler.subtopic_ids)}")
    print(f"Unique main topics found: {len(all_main_topics)} / {len(labeler.main_topics)}")
    
    # Show sample labeled sections
    print("\n" + "=" * 80)
    print("ðŸ“‹ SAMPLE LABELED SECTIONS (first document)")
    print("=" * 80)
    
    first_doc = list(results.values())[0]
    print(f"\nDocument: {first_doc['doc_id']}")
    print(f"Sections: {first_doc['num_sections']}, Labeled: {first_doc['num_labeled_sections']}")
    
    for section in first_doc['sections'][:5]:  # First 5 sections
        print(f"\n  Section {section['section_idx']}:")
        print(f"  Text preview: {section['text'][:100]}...")
        
        if section['labels']['primary_subtopic']:
            labels = section['labels']
            print(f"  â†’ Subtopic: {labels['primary_subtopic']['name']} ({labels['primary_subtopic']['id']})")
            print(f"  â†’ Main Topic: {labels['primary_main_topic']['name']} ({labels['primary_main_topic']['id']})")
            print(f"  â†’ Confidence: {labels['primary_subtopic']['confidence']:.3f}")
        else:
            print(f"  â†’ No label (below threshold)")
    
    return results, labeler


if __name__ == "__main__":
    # Run labeling using global variables
    results, labeler = asyncio.run(process_all_documents(
        input_dir=INPUT_DIR,
        taxonomy_file=TAXONOMY_FILE,
        output_file=OUTPUT_FILE,
        limit=LIMIT
    ))
    
